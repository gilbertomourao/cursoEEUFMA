{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364228bf",
   "metadata": {},
   "source": [
    "# Classificação de palavras com LSTM\n",
    "\n",
    "Na linguagem natural, classificar uma palavra como artigo, verbo ou nome é uma tarefa extremamente simples. Há casos em que uma palavra pode ter dois significados diferentes em uma frase, dependendo do contexto. A palavra \"um\" por exemplo pode significar um numeral ou um artigo indefinido. Para nós, é extremamente simples idetificar o contexto no qual uma palavra foi aplicada, uma vez que basta analisarmos o restante da frase. Para um computador, essa tarefa não é tão simples. Como o significado de uma palavra depende da frase na qual ela foi utilizada, a utilização de RNN torna-se uma boa escolha para fazer a análise da sequência (nesse caso, a frase). Neste exemplo, originalmente feito por https://github.com/LeanManager/NLP-PyTorch/blob/master/LSTM%20Speech%20Tagging%20with%20PyTorch.ipynb, utilizaremos a arquitetura LSTM para classificação de palavras em artigo, nome ou verbo. Será utilizada uma base de dados extremamente simples, pois o importante é apenas entender o funcionamento da rede LSTM.\n",
    "\n",
    "## Dados para treinamento\n",
    "\n",
    "A nossa base de dados tem a forma mostrada no arquivo frases.txt.\n",
    "\n",
    "A primeira linha é um dicionário contendo as possíveis tags, a segunda linha está vazia e as demais contém as frases e as tags de cada palavra. Tomando a primeira frase como exemplo:\n",
    "\n",
    "O gato comeu o queijo\n",
    "\n",
    "O -> artigo\n",
    "gato -> nome\n",
    "comeu -> verbo\n",
    "o -> artigo\n",
    "queijo -> nome\n",
    "\n",
    "A base de dados também contém algumas frases com advérbios e numerais, bem como frases sem advérbio e com artigos indefinidos. Esta foi uma maneira extremamente simplista de tentar dizer para a rede que quando há um advérbio antes de \"um\", então ele é um número. Caso contrário, um artigo indefinido. \n",
    "\n",
    "Redes neurais não costumam se dar bem com palavras como dados de entrada. Portanto, em primeiro lugar iremos transformar sequências de palavras, ou frases, em vetores de números. Como utilizaremos o pytorch, faremos o uso de tensores para realizar tal tarefa. \n",
    "\n",
    "Para facilitar o uso da rede, implementamos tudo em uma classe LSTMTagger, como foi originalmente batizada pelo autor. Antes de mostrar a implementação, vamos ver como o modelo deve se comportar.\n",
    "\n",
    "## Criando o modelo\n",
    "\n",
    "Nosso modelo assume que as seguintes condições são satisfeitas:\n",
    "\n",
    "1. A entrada é uma sequência de palavras do tipo [w1, w2, ..., wn]\n",
    "2. As palavras na entrada vêm de uma frase (ou uma string)\n",
    "3. Temos um número de tags limitado: ART, NN, V (Artigo, Nome, Verbo\n",
    "4. Queremos prever uma tag para cada palavra na entrada\n",
    "\n",
    "Para realizar a previsão (ou predição, considerando que a saída será mostrada ou \"dita\" pela rede), utilizaremos uma LSTM para uma sequência de testes e aplicaremos a função softmax ao hidden state. O resultato deverá ser um tensor com a pontuação das tags a partir do qual poderemos predizer a tag de uma palavra baseado no máximo valor apresentado nas pontuações obtidas.\n",
    "\n",
    "Matematicamente, podemos representar uma previsão de tag da seguinte maneira:\n",
    "\n",
    "$$\\hat{y}_i = \\text{argmax}_j (\\text{logSoftmax}(Ah_i + b))_j$$\n",
    "\n",
    "Onde $A$ é um peso aprendido pelo modelo, $b$ um termo de bias aprendido e $h_i$ o hidden state no tempo $i$.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "A rede LSTM recebe como argumento um tamanho de entrada e um tamanho para o hidden state. Porém, raramente as sentenças possuem tamanho fixo. Para contornar isso, pode ser utilizada uma camada de associação logo após a entrada. Essa camada é capaz de receber uma palavra e gerar um tensor numérico de tamanho definido que a representa. Assim, ainda que a palavra varie de tamanho, após passar pela camada de associação, ela terá o tamanho definido previamente pelo usuário. \n",
    "\n",
    "Após passar pela camada de associação, os tensores são tratados pela camada LSTM e por fim por uma camada linear. Essa basicamente faz uma transformação linear na saída do hidden state da LSTM para que ela saia com as dimensões especificadas pelo usuário. Nesse caso, a saída para cada palavra deverá ser um vetor de 3 posições, cada uma contendo uma pontuação para cada tag: [ART, NN, V].\n",
    "\n",
    "![Arquitetura do modelo](Imagens/Arquitetura.png)\n",
    "\n",
    "Agora que já vimos a arquitetura do modelo, vamos para a implementação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9889b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, device, embedding_dim, hidden_dim, training_data_file):\n",
    "        ''' Incializa as camadas do modelo'''\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        # verifica se device está ok\n",
    "        if device == \"gpu\":\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # Verifica se encontrou uma gpu disponível\n",
    "            if self.device == \"cpu\":\n",
    "                warnings.warn('ATENÇÃO: Nenhuma gpu foi encontrada!')\n",
    "\n",
    "        elif device == \"cpu\":\n",
    "            self.device = \"cpu\"\n",
    "        else:\n",
    "            raise ValueError('Nenhum dispositivo com essa nomenclatura pode ser reconhecido pelo modelo.')\n",
    "\n",
    "        # A inicialização do modelo já armazena os dados de treinamento nele. \n",
    "        self.training_data, self.word2idx, self.tag2idx = self.read_training_data(training_data_file)\n",
    "\n",
    "        # Obtém o tamanho do vocabulário, ou seja, a quantidade de palavras \n",
    "        # contempladas pelo modelo.\n",
    "        vocab_size = len(self.word2idx)\n",
    "\n",
    "        # Obtém o tamanho do conjunto de tags, ou seja, a quantidade de tags \n",
    "        # contempladas pelo modelo. \n",
    "        tagset_size = len(self.tag2idx)\n",
    "        \n",
    "        # Inicializa a dimensão do hidden layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Camada de associação que obtém vetores de um tamanho específico a partir\n",
    "        # de uma palavra\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim).to(self.device)\n",
    "\n",
    "        # A LSTM recebe os vetores da camada de associação como entrada e sai \n",
    "        # com os hidden states de tamanho hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(self.device)\n",
    "\n",
    "        # Camada linear que mapeia a dimensão do hidden state para o número \n",
    "        # de tags que se deseja obter na saída, ou seja, tagset_size\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size).to(self.device)\n",
    "        \n",
    "        # Inicializa o hidden state\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''\n",
    "        No início do treinamento, o modelo precisa inicializar o hidden state. \n",
    "        Como ele é formado basicamente por dados anteriores (h[t-1]), no início \n",
    "        não haverá qualquer hidden state. Esta função define um hidden state \n",
    "        inicial como um tensor de zeros.\n",
    "        '''\n",
    "        # As dimensões dos tensores são (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim).to(self.device),\n",
    "                torch.zeros(1, 1, self.hidden_dim).to(self.device))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        ''' Define o comportamento feedfoward do modelo '''\n",
    "        # cria vetores para cada palavra na sentença através da camada de associação (embedding)\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # obtém a saída e o hidden state aplicando os vetores das embedded words \n",
    "        # e o hidden state anterior.\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # Obtém a pontuação para a tag mais provável utilizando a função softmax\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        \"\"\"\n",
    "        Treina o modelo\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a função de perda e o otimizador.\n",
    "        # Nesse caso, são utilizadas as funções negative log likelihood e \n",
    "        # stochastic gradient descent.\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "    \n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            # Obtém todas as sentenças e tags correspondentes nos dados de treinamento\n",
    "            for sentence, tags in self.training_data:\n",
    "                \n",
    "                # zera os gradientes\n",
    "                self.zero_grad()\n",
    "\n",
    "                # zera o hidden state da LSTM, ou seja, reseta a história da mesma\n",
    "                self.hidden = self.init_hidden()\n",
    "\n",
    "                # prepara as entradas para o processamento pela rede, \n",
    "                # transforma todas as sentenças e targets em tensores numéricos\n",
    "                sentence_in = self.prepare_sequence(sentence, self.word2idx)\n",
    "                targets = self.prepare_sequence(tags, self.tag2idx)\n",
    "\n",
    "                # forward pass para obter a pontuação das tags\n",
    "                tag_scores = self(sentence_in)\n",
    "\n",
    "                # calcula o loss e o gradiente\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                \n",
    "                # atualiza os parâmetros do modelo\n",
    "                optimizer.step()\n",
    "                \n",
    "            # Imprime a loss/perda média a cada 20 épocas.\n",
    "            if(epoch%20 == 19):\n",
    "                print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(self.training_data)))\n",
    "\n",
    "    def read_training_data(self, file):\n",
    "        \"\"\"\n",
    "        Função para ler os dados para treinamento. As frases obtidas são divididas \n",
    "        em uma lista de palavras, onde cada palavra possui uma tag já associada. \n",
    "        \"\"\"\n",
    "        f = open(file)\n",
    "        first_line = f.readline()\n",
    "\n",
    "        tag2idx = ast.literal_eval(first_line)\n",
    "\n",
    "        second_line = f.readline()\n",
    "        lines = f.readlines()\n",
    "\n",
    "        training_data = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.split(', [')\n",
    "            words = line[0].split()\n",
    "            tags = ast.literal_eval('[' + line[1])\n",
    "            training_data.append((words, tags))\n",
    "\n",
    "        word2idx = {}\n",
    "\n",
    "        for sent, tags in training_data:\n",
    "            for word in sent:\n",
    "                if word not in word2idx:\n",
    "                    word2idx[word] = len(word2idx)\n",
    "\n",
    "        return training_data, word2idx, tag2idx\n",
    "\n",
    "    def prepare_sequence(self, seq, to_idx):\n",
    "        \"\"\"\n",
    "        Esta função recebe uma sequência de palavras e retorna um tensor \n",
    "        correspondente de valores numéricos (índices de cada palavra).\n",
    "        \"\"\"\n",
    "        \n",
    "        idxs = [to_idx[w] for w in seq]\n",
    "        idxs = np.array(idxs)\n",
    "        \n",
    "        tensor = torch.from_numpy(idxs).long()\n",
    "\n",
    "        tensor = tensor.to(self.device)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def predict_tags(self, sentence):\n",
    "        \"\"\"\n",
    "        Prediz as tags de cada palavra de uma sentença.\n",
    "        \"\"\"\n",
    "\n",
    "        seq = sentence.lower().split()\n",
    "\n",
    "        inputs = self.prepare_sequence(seq, self.word2idx)\n",
    "        tag_scores = self(inputs)\n",
    "        print(tag_scores)\n",
    "\n",
    "        # Obtém os índices com maior pontuação\n",
    "        _, predicted_tags = torch.max(tag_scores, 1)\n",
    "\n",
    "        # Converte os números em tags reais para melhor visualização\n",
    "\n",
    "        tags = []\n",
    "\n",
    "        for tag_id in predicted_tags:\n",
    "            key = list(self.tag2idx.keys())[tag_id]\n",
    "            tags.append(key)\n",
    "\n",
    "        print('\\n')\n",
    "        print('Predicted tags: \\n',tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c212b",
   "metadata": {},
   "source": [
    "Vamos instanciar o nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9a798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM define o tamanho do vetor da palavra.\n",
    "# Neste exemplo, com vocabulário simples e um conjunto de treinamento muito limitado, \n",
    "# resolvemos manter os valores das dimensões pequenos. \n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Instancia o modelo\n",
    "model = LSTMTagger(\"gpu\", EMBEDDING_DIM, HIDDEN_DIM, 'frases.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f3d6",
   "metadata": {},
   "source": [
    "Observe que o modelo lê automaticamente a base de dados de treinamento quando instanciado. Após ler os dados desejados, ele cria índices para cada palavra e para cada tag, como definido na base de dados. Além disso, ele obtém os dados para treinamento como uma lista de tuplas. Cada tupla contém duas listas, uma contendo as palavras de cada sentença e a outra contendo as respectivas tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a2a797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['O', 'gato', 'comeu', 'o', 'queijo'], ['ART', 'NN', 'V', 'ART', 'NN']), (['Ela', 'leu', 'aquele', 'livro'], ['NN', 'V', 'ART', 'NN']), (['O', 'cachorro', 'ama', 'arte'], ['ART', 'NN', 'V', 'NN']), (['O', 'elefante', 'atende', 'o', 'telefone'], ['ART', 'NN', 'V', 'ART', 'NN']), (['O', 'rapaz', 'precisa', 'de', 'um', 'gato'], ['ART', 'NN', 'V', 'PP', 'ART', 'NN']), (['O', 'rapaz', 'mora', 'com', 'uma', 'amiga'], ['ART', 'NN', 'V', 'PP', 'ART', 'NN']), (['O', 'garoto', 'pegou', 'um', 'item'], ['ART', 'NN', 'V', 'ART', 'NN']), (['A', 'garota', 'pegou', 'somente', 'um', 'item'], ['ART', 'NN', 'V', 'ADV', 'NUM', 'NN']), (['O', 'garoto', 'pegou', 'mais', 'de', 'um', 'item'], ['ART', 'NN', 'V', 'ADV', 'PP', 'NUM', 'NN']), (['A', 'garota', 'falou', 'com', 'mais', 'de', 'uma', 'pessoa'], ['ART', 'NN', 'V', 'PP', 'ADV', 'PP', 'NUM', 'NN'])] \n",
      "---\n",
      " {'O': 0, 'gato': 1, 'comeu': 2, 'o': 3, 'queijo': 4, 'Ela': 5, 'leu': 6, 'aquele': 7, 'livro': 8, 'cachorro': 9, 'ama': 10, 'arte': 11, 'elefante': 12, 'atende': 13, 'telefone': 14, 'rapaz': 15, 'precisa': 16, 'de': 17, 'um': 18, 'mora': 19, 'com': 20, 'uma': 21, 'amiga': 22, 'garoto': 23, 'pegou': 24, 'item': 25, 'A': 26, 'garota': 27, 'somente': 28, 'mais': 29, 'falou': 30, 'pessoa': 31} \n",
      "---\n",
      " {'ART': 0, 'NN': 1, 'V': 2, 'NUM': 3, 'ADV': 4, 'PP': 5}\n"
     ]
    }
   ],
   "source": [
    "# Obtém os dados e imprime\n",
    "training_data, word2idx, tag2idx = model.read_training_data('frases.txt')\n",
    "print(training_data,'\\n---\\n',word2idx,'\\n---\\n',tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ba736",
   "metadata": {},
   "source": [
    "Para trabalhar com o pytorch, precisamos utilizar tensores. Para isso, criamos uma função que recebe a frase e o dicionário de índices. A seguir, a função cria um vetor de índices, transforma em um numpy array e por fim em um tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118db015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3, 12, 13,  3, 14], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# verifica o que a função prepare_sequence faz com uma das sentenças que serão \n",
    "# utilizadas no treinamento\n",
    "example_input = model.prepare_sequence(\"O elefante atende o telefone\".lower().split(), model.word2idx)\n",
    "\n",
    "print(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1db29",
   "metadata": {},
   "source": [
    "Para verificar como o modelo se comporta, realizaremos um teste. Definiremos uma sentença com palavras existentes em nossa base de dados anteriormente definida. A seguir, utilizaremos o método predict_tags do modelo para verificar a quais tags ele está associando cada palavra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3a7c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6251, -1.8151, -2.2346, -1.6311, -1.6082, -1.9856],\n",
      "        [-1.6707, -1.7463, -2.2539, -1.6620, -1.6546, -1.8867],\n",
      "        [-1.6469, -1.7890, -2.2507, -1.6516, -1.6009, -1.9559],\n",
      "        [-1.6560, -1.8006, -2.2284, -1.6339, -1.5993, -1.9733],\n",
      "        [-1.6775, -1.8015, -2.2301, -1.5655, -1.6365, -1.9882]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " ['ADV', 'ADV', 'ADV', 'ADV', 'NUM']\n"
     ]
    }
   ],
   "source": [
    "# verifica como o modelo se sai antes do treinamento\n",
    "model.predict_tags(\"O queijo ama o elefante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce9633",
   "metadata": {},
   "source": [
    "Para este exemplo, o nosso modelo classificou todas as palavras da frase \"O queijo ama o elefante\" como advérbios. Sabemos que isso está incorreto e por isso devemos treinar o modelo. No entanto, ele parece estar se comportando como queríamos no início. \n",
    "\n",
    "Para o treinamento, utilizamos a função de perda NLL e a função de otimização SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf3ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, loss: 1.35293\n",
      "Epoch: 40, loss: 0.97541\n",
      "Epoch: 60, loss: 0.63286\n",
      "Epoch: 80, loss: 0.40344\n",
      "Epoch: 100, loss: 0.26964\n",
      "Epoch: 120, loss: 0.19055\n",
      "Epoch: 140, loss: 0.14054\n",
      "Epoch: 160, loss: 0.10526\n",
      "Epoch: 180, loss: 0.08002\n",
      "Epoch: 200, loss: 0.06277\n",
      "Epoch: 220, loss: 0.05065\n",
      "Epoch: 240, loss: 0.04188\n",
      "Epoch: 260, loss: 0.03537\n",
      "Epoch: 280, loss: 0.03043\n",
      "Epoch: 300, loss: 0.02660\n"
     ]
    }
   ],
   "source": [
    "# Treina o modelo\n",
    "n_epochs = 300\n",
    "model.train(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc311267",
   "metadata": {},
   "source": [
    "Realizaremos dois testes com o modelo treinado. O primeiro será com a mesma frase de antes. O segundo será com uma frase envolvendo a palavra \"uma\". Vamos ver se o modelo irá acertar na sua escolha de tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ad215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6363e-01, -1.5963e+00, -4.9171e+00, -4.0909e+00, -5.3005e+00,\n",
      "         -9.7924e+00],\n",
      "        [-6.9394e+00, -3.7683e-02, -3.3907e+00, -1.0154e+01, -6.0974e+00,\n",
      "         -1.0141e+01],\n",
      "        [-3.3881e+00, -5.5340e+00, -4.0873e-02, -8.4815e+00, -6.5227e+00,\n",
      "         -7.3415e+00],\n",
      "        [-3.8628e-03, -6.4373e+00, -6.1776e+00, -8.7467e+00, -1.1227e+01,\n",
      "         -1.1900e+01],\n",
      "        [-7.6810e+00, -2.6280e-02, -3.6789e+00, -1.1570e+01, -8.5295e+00,\n",
      "         -1.0960e+01]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " ['ART', 'NN', 'V', 'ART', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# verifica como o modelo se sai depois de treinado\n",
    "model.predict_tags(\"O queijo ama o elefante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd5bb8",
   "metadata": {},
   "source": [
    "No primeiro teste após o treino, o modelo teve sucesso na classificação das palavras. Vamos ver como ele se sairá no segundo teste.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ebc32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8689e-02, -4.5763e+00, -4.8660e+00, -7.8221e+00, -9.1739e+00,\n",
      "         -1.1237e+01],\n",
      "        [-9.5435e+00, -4.0073e-02, -3.2793e+00, -1.1746e+01, -6.5314e+00,\n",
      "         -9.3141e+00],\n",
      "        [-5.6767e+00, -4.3971e+00, -2.4783e-02, -9.2788e+00, -5.6359e+00,\n",
      "         -5.2825e+00],\n",
      "        [-3.9262e+00, -7.3387e+00, -2.3203e+00, -6.0578e+00, -5.3785e+00,\n",
      "         -1.3418e-01],\n",
      "        [-7.5519e-03, -7.1979e+00, -7.7570e+00, -5.1939e+00, -8.7966e+00,\n",
      "         -7.3443e+00],\n",
      "        [-3.3780e+00, -4.6409e-02, -6.2302e+00, -5.8212e+00, -5.0758e+00,\n",
      "         -9.8332e+00]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " ['ART', 'NN', 'V', 'PP', 'ART', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# Realiza um novo teste\n",
    "model.predict_tags(\"O garoto falou com uma pessoa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30952c",
   "metadata": {},
   "source": [
    "Observe que a rede teve sucesso na classificação da palavra \"uma\". Isso aconteceu pelo fato de termos inserido frases na base de dados que sugerem essa escolha. Por não haver advérbio antes de \"uma\", entende-se que se trata de um artigo indefinido. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19932c69",
   "metadata": {},
   "source": [
    "## Finalizando\n",
    "\n",
    "O modelo se comportou bem e conseguiu acertar as tags de cada palavra. Foi um exemplo extremamente simples, mas que mostrou o funcionamento da LSTM. Para exemplos maiores e mais complexos, uma base de dados maior poderia ser utilizada. Assim, vimos que uma rede LSTM pode ser utilizada para classificar palavras como artigo, verbo e nome, mas não para por aí. Redes que utilizam arquitetura semelhante podem por exemplo traduzir textos! O google tradutor funciona dessa maneira. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
