{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364228bf",
   "metadata": {},
   "source": [
    "# Classificação de palavras com LSTM\n",
    "\n",
    "Na linguagem natural, classificar uma palavra como artigo, verbo ou nome é uma tarefa extremamente simples. Há casos em que uma palavra pode ter dois significados diferentes em uma frase, dependendo do contexto. Para nós, é extremamente simples idetificar o contexto no qual uma palavra foi aplicada, uma vez que basta analisarmos o restante da frase. Para um computador, essa tarefa não é tão simples. Como o significado de uma palavra depende da frase na qual ela foi utilizada, a utilização de RNN torna-se uma boa escolha para fazer a análise da sequência (nesse caso, a frase). Neste exemplo, originalmente feito por https://github.com/LeanManager/NLP-PyTorch/blob/master/LSTM%20Speech%20Tagging%20with%20PyTorch.ipynb, utilizaremos a arquitetura LSTM para classificação de palavras em artigo, nome ou verbo. Será utilizada uma base de dados extremamente simples, pois o importante é apenas entender o funcionamento da rede LSTM.\n",
    "\n",
    "## Dados para treinamento\n",
    "\n",
    "Redes neurais não costumam se dar bem com palavras como dados de entrada. Portanto, em primeiro lugar iremos transformar sequências de palavras, ou frases, em vetores de números. Como utilizaremos o pytorch, faremos o uso de tensores para realizar tal tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9889b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define o dispositivo que será utilizado (CPU ou GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f3d6",
   "metadata": {},
   "source": [
    "As frases que utilizaremos são extremamente simples. Primeiro, as strings serão separadas em um vetor de palavras. A seguir, será criado um dicionário para armazenar os índices correspondentes de cada palavra na nossa base de dados. O mesmo será feito com as tags. No presente exemplo, trataremos artigos, nomes e verbos, como no código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a2a797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'gato': 1, 'comeu': 2, 'queijo': 3, 'ela': 4, 'leu': 5, 'aquele': 6, 'livro': 7, 'cachorro': 8, 'ama': 9, 'arte': 10, 'elefante': 11, 'atende': 12, 'telefone': 13}\n",
      "{'ART': 0, 'NN': 1, 'V': 2}\n"
     ]
    }
   ],
   "source": [
    "# setenças para treinamento e as classificações de suas palavras\n",
    "# ART = artigo\n",
    "# NN = nome\n",
    "# V = verbo\n",
    "training_data = [\n",
    "\t(\"O gato comeu o queijo\".lower().split(), [\"ART\", \"NN\", \"V\", \"ART\", \"NN\"]),\n",
    "    (\"Ela leu aquele livro\".lower().split(), [\"NN\", \"V\", \"ART\", \"NN\"]),\n",
    "    (\"O cachorro ama arte\".lower().split(), [\"ART\", \"NN\", \"V\", \"NN\"]),\n",
    "    (\"O elefante atende o telefone\".lower().split(), [\"ART\", \"NN\", \"V\", \"ART\", \"NN\"])\n",
    "]\n",
    "\n",
    "# cria um dicionário para representação numérica das palavras\n",
    "# palavras -> índices\n",
    "word2idx = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "print(word2idx)\n",
    "\n",
    "# cria um dicionário para representação numérica da classificação das palavras\n",
    "tag2idx = {\"ART\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ba736",
   "metadata": {},
   "source": [
    "Para trabalhar com o pytorch, precisamos utilizar tensores. Para isso, criaremos uma função que recebe a frase e o dicionário de índices. A seguir, a função cria um vetor de índices, transforma em um numpy array e por fim em um tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "118db015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 11, 12,  0, 13], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Função que converte uma sequência de palavras em um tensor de valores numéricos\n",
    "# Será usada para treinamento\n",
    "def prepare_sequence(seq, to_idx):\n",
    "    \"\"\"\n",
    "    Esta função recebe uma sequência de palavras e retorna um tensor \n",
    "    correspondente de valores numéricos (índices de cada palavra).\n",
    "    \"\"\"\n",
    "    \n",
    "    idxs = [to_idx[w] for w in seq]\n",
    "    idxs = np.array(idxs)\n",
    "    \n",
    "    tensor = torch.from_numpy(idxs).long()\n",
    "\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# verifica o que a função prepare_sequence faz com uma das sentenças que serão \n",
    "# utilizadas no treinamento\n",
    "example_input = prepare_sequence(\"O elefante atende o telefone\".lower().split(), word2idx)\n",
    "\n",
    "print(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1db29",
   "metadata": {},
   "source": [
    "## Criando o modelo\n",
    "\n",
    "Nosso modelo assume que as seguintes condições são satisfeitas:\n",
    "\n",
    "1. A entrada é uma sequência de palavras do tipo [w1, w2, ..., wn]\n",
    "2. As palavras na entrada vêm de uma frase (ou uma string)\n",
    "3. Temos um número de tags limitado: ART, NN, V (Artigo, Nome, Verbo\n",
    "4. Queremos prever uma tag para cada palavra na entrada\n",
    "\n",
    "Para realizar a previsão (ou predição, considerando que a saída será mostrada ou \"dita\" pela rede), utilizaremos uma LSTM para uma sequência de testes e aplicaremos a função softmax ao hidden state. O resultato deverá ser um tensor com a pontuação das tags a partir do qual poderemos predizer a tag de uma palavra baseado no máximo valor apresentado nas pontuações obtidas.\n",
    "\n",
    "Matematicamente, podemos representar uma previsão de tag da seguinte maneira:\n",
    "\n",
    "$$\\hat{y}_i = \\text{argmax}_j (\\text{logSoftmax}(Ah_i + b))_j$$\n",
    "\n",
    "Onde $A$ é um peso aprendido pelo modelo, $b$ um termo de bias aprendido e $h_i$ o hidden state no tempo $i$.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "A rede LSTM recebe como argumento um tamanho de entrada e um tamanho para o hidden state. Porém, raramente as sentenças possuem tamanho fixo. Para contornar isso, pode ser utilizada uma camada de associação logo após a entrada. Essa camada é capaz de receber uma palavra e gerar um tensor numérico de tamanho definido que a representa. Assim, ainda que a palavra varie de tamanho, após passar pela camada de associação, ela terá o tamanho definido previamente pelo usuário. \n",
    "\n",
    "Após passar pela camada de associação, os tensores são tratados pela camada LSTM e por fim por uma camada linear. Essa basicamente faz uma transformação linear na saída do hidden state da LSTM para que ela saia com as dimensões especificadas pelo usuário. Nesse caso, a saída para cada palavra deverá ser um vetor de 3 posições, cada uma contendo uma pontuação para cada tag: [ART, NN, V].\n",
    "\n",
    "![Arquitetura do modelo](Imagens/Arquitetura.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3a7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs \n",
    "        # and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "        # the linear layer that maps the hidden state output dimension \n",
    "        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size).to(device)\n",
    "        \n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        ''' At the start of training, we need to initialize a hidden state;\n",
    "           there will be none because the hidden state is formed based on perviously seen data.\n",
    "           So, this function defines a hidden state with all zeroes and of a specified size.'''\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim).to(device),\n",
    "                torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        ''' Define the feedforward behavior of the model.'''\n",
    "        # create embedded word vectors for each word in a sentence\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # get the output and hidden state by passing the lstm over our word embeddings\n",
    "        # the lstm takes in our embeddings and hiddent state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # get the scores for the most likely tag for a word\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce9633",
   "metadata": {},
   "source": [
    "Para este exemplo, por ser muito simples, podemos utilizar dimensões reduzidas para a camada de associação e para o hidden state da LSTM. O tamanho do vocabulário nada mais é do que o tamanho do dicionário de índices definido anteriormente. A partir dele a camada de associação será capaz de construir o tensor que representa a palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf3ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O dimensão da camada de embedding define o tamanho do vetor que representa cada palavra.\n",
    "# Por se tratar de um exemplo extremamente simples, serão consideradas dimensões pequenas. \n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# instantiate our model\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc311267",
   "metadata": {},
   "source": [
    "Para verificar como o modelo se comporta, realizaremos um teste. Definiremos uma sentença com palavras existentes em nossa base de dados anteriormente definida. A seguir, será obtida a sequência de índices e por fim o modelo será utilizado para verificar a pontuação de cada palavra para cada tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ad215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9782, -1.3721, -0.9930],\n",
      "        [-0.9247, -1.4035, -1.0283],\n",
      "        [-0.9336, -1.3931, -1.0257],\n",
      "        [-0.9334, -1.4338, -0.9986],\n",
      "        [-1.0253, -1.2551, -1.0321]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"O queijo ama o elefante\".lower().split()\n",
    "\n",
    "inputs = prepare_sequence(test_sentence, word2idx)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)\n",
    "\n",
    "# Obtém as tags com maior pontuação\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd5bb8",
   "metadata": {},
   "source": [
    "O modelo parece estar se comportanto exatamente como queremos. Agora, precisa ser treinado. Para isso, definiremos a função de perda Negative Log Likelihood (NLL) e a função de otimização Stochastic Gradient Descent (SGD). Por se tratar de um modelo simples, o número de épocas pode ser pequeno. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ebc32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, loss: 0.94513\n",
      "Epoch: 40, loss: 0.72202\n",
      "Epoch: 60, loss: 0.46925\n",
      "Epoch: 80, loss: 0.28682\n",
      "Epoch: 100, loss: 0.17411\n",
      "Epoch: 120, loss: 0.11429\n",
      "Epoch: 140, loss: 0.08104\n",
      "Epoch: 160, loss: 0.06094\n",
      "Epoch: 180, loss: 0.04782\n",
      "Epoch: 200, loss: 0.03870\n",
      "Epoch: 220, loss: 0.03205\n",
      "Epoch: 240, loss: 0.02704\n",
      "Epoch: 260, loss: 0.02315\n",
      "Epoch: 280, loss: 0.02009\n",
      "Epoch: 300, loss: 0.01764\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # get all sentences and corresponding tags in the training data\n",
    "    for sentence, tags in training_data:\n",
    "        \n",
    "        # zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # zero the hidden state of the LSTM, this ARTaches it from its history\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # prepare the inputs for processing by out network, \n",
    "        # turn all sentences and targets into Tensors of numerical indices\n",
    "        sentence_in = prepare_sequence(sentence, word2idx)\n",
    "        targets = prepare_sequence(tags, tag2idx)\n",
    "\n",
    "        # forward pass to get tag scores\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # compute the loss, and gradients \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the model parameters with optimizer.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print out avg loss per 20 epochs\n",
    "    if(epoch%20 == 19):\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30952c",
   "metadata": {},
   "source": [
    "Com o modelo treinado, realizaremos um teste. Por se tratar de um exemplo extremamente simples, espera-se que o modelo se saia bem. A frase escolhida foi \"O queijo ama o elefante\". Observe que 'O' é um artigo, 'queijo' um nome, 'ama' um verbo, 'o' um artigo e 'elefante' um nome. Assim, a classificação correta é: [0, 1, 2, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365177ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.6825e-03, -6.0592e+00, -6.0596e+00],\n",
      "        [-6.5345e+00, -1.0237e-02, -4.7407e+00],\n",
      "        [-4.8942e+00, -4.1038e+00, -2.4293e-02],\n",
      "        [-1.3391e-02, -6.0029e+00, -4.5254e+00],\n",
      "        [-6.3960e+00, -1.4984e-02, -4.3272e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([0, 1, 2, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"O queijo ama o elefante\".lower().split()\n",
    "\n",
    "# Verifica a pontuação após o treinamento\n",
    "inputs = prepare_sequence(test_sentence, word2idx)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)\n",
    "\n",
    "# Obtém a pontuação máxima para cada tag\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19932c69",
   "metadata": {},
   "source": [
    "## Finalizando\n",
    "\n",
    "O modelo se comportou bem e conseguiu acertar as tags de cada palavra. Foi um exemplo extremamente simples, mas que mostrou o funcionamento da LSTM. Para exemplos maiores e mais complexos, uma base de dados maior poderia ser utilizada. Assim, vimos que uma rede LSTM pode ser utilizada para classificar palavras como artigo, verbo e nome, mas não para por aí. Redes que utilizam arquitetura semelhante podem por exemplo traduzir textos! O google tradutor funciona dessa maneira. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
